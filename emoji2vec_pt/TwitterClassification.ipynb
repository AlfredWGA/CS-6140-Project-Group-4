{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n",
      "Requirement already satisfied: tweepy in c:\\users\\sandeep bhatula\\appdata\\local\\programs\\python\\python38\\lib\\site-packages (4.4.0)\n",
      "Requirement already satisfied: requests-oauthlib<2,>=1.0.0 in c:\\users\\sandeep bhatula\\appdata\\local\\programs\\python\\python38\\lib\\site-packages (from tweepy) (1.3.0)\n",
      "Requirement already satisfied: requests<3,>=2.11.1 in c:\\users\\sandeep bhatula\\appdata\\local\\programs\\python\\python38\\lib\\site-packages (from tweepy) (2.25.1)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in c:\\users\\sandeep bhatula\\appdata\\local\\programs\\python\\python38\\lib\\site-packages (from requests<3,>=2.11.1->tweepy) (1.26.4)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\sandeep bhatula\\appdata\\local\\programs\\python\\python38\\lib\\site-packages (from requests<3,>=2.11.1->tweepy) (2020.12.5)\n",
      "Requirement already satisfied: chardet<5,>=3.0.2 in c:\\users\\sandeep bhatula\\appdata\\local\\programs\\python\\python38\\lib\\site-packages (from requests<3,>=2.11.1->tweepy) (4.0.0)\n",
      "Requirement already satisfied: idna<3,>=2.5 in c:\\users\\sandeep bhatula\\appdata\\local\\programs\\python\\python38\\lib\\site-packages (from requests<3,>=2.11.1->tweepy) (2.10)\n",
      "Requirement already satisfied: oauthlib>=3.0.0 in c:\\users\\sandeep bhatula\\appdata\\local\\programs\\python\\python38\\lib\\site-packages (from requests-oauthlib<2,>=1.0.0->tweepy) (3.1.0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Value for scheme.headers does not match. Please report this to <https://github.com/pypa/pip/issues/9617>\n",
      "distutils: c:\\users\\sandeep bhatula\\appdata\\local\\programs\\python\\python38\\Include\\UNKNOWN\n",
      "sysconfig: c:\\users\\sandeep bhatula\\appdata\\local\\programs\\python\\python38\\Include\n",
      "WARNING: Additional context:\n",
      "user = False\n",
      "home = None\n",
      "root = None\n",
      "prefix = None\n",
      "WARNING: Value for scheme.headers does not match. Please report this to <https://github.com/pypa/pip/issues/9617>\n",
      "distutils: c:\\users\\sandeep bhatula\\appdata\\local\\programs\\python\\python38\\Include\\UNKNOWN\n",
      "sysconfig: c:\\users\\sandeep bhatula\\appdata\\local\\programs\\python\\python38\\Include\n",
      "WARNING: Additional context:\n",
      "user = False\n",
      "home = None\n",
      "root = None\n",
      "prefix = None\n",
      "WARNING: You are using pip version 21.1; however, version 21.3.1 is available.\n",
      "You should consider upgrading via the 'c:\\users\\sandeep bhatula\\appdata\\local\\programs\\python\\python38\\python.exe -m pip install --upgrade pip' command.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: sklearn in c:\\users\\sandeep bhatula\\appdata\\local\\programs\\python\\python38\\lib\\site-packages (0.0)\n",
      "Requirement already satisfied: scikit-learn in c:\\users\\sandeep bhatula\\appdata\\local\\programs\\python\\python38\\lib\\site-packages (from sklearn) (0.24.2)\n",
      "Requirement already satisfied: scipy>=0.19.1 in c:\\users\\sandeep bhatula\\appdata\\local\\programs\\python\\python38\\lib\\site-packages (from scikit-learn->sklearn) (1.6.3)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in c:\\users\\sandeep bhatula\\appdata\\local\\programs\\python\\python38\\lib\\site-packages (from scikit-learn->sklearn) (2.1.0)\n",
      "Requirement already satisfied: numpy>=1.13.3 in c:\\users\\sandeep bhatula\\appdata\\local\\programs\\python\\python38\\lib\\site-packages (from scikit-learn->sklearn) (1.20.2)\n",
      "Requirement already satisfied: joblib>=0.11 in c:\\users\\sandeep bhatula\\appdata\\local\\programs\\python\\python38\\lib\\site-packages (from scikit-learn->sklearn) (1.0.1)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Value for scheme.headers does not match. Please report this to <https://github.com/pypa/pip/issues/9617>\n",
      "distutils: c:\\users\\sandeep bhatula\\appdata\\local\\programs\\python\\python38\\Include\\UNKNOWN\n",
      "sysconfig: c:\\users\\sandeep bhatula\\appdata\\local\\programs\\python\\python38\\Include\n",
      "WARNING: Additional context:\n",
      "user = False\n",
      "home = None\n",
      "root = None\n",
      "prefix = None\n",
      "WARNING: Value for scheme.headers does not match. Please report this to <https://github.com/pypa/pip/issues/9617>\n",
      "distutils: c:\\users\\sandeep bhatula\\appdata\\local\\programs\\python\\python38\\Include\\UNKNOWN\n",
      "sysconfig: c:\\users\\sandeep bhatula\\appdata\\local\\programs\\python\\python38\\Include\n",
      "WARNING: Additional context:\n",
      "user = False\n",
      "home = None\n",
      "root = None\n",
      "prefix = None\n",
      "WARNING: You are using pip version 21.1; however, version 21.3.1 is available.\n",
      "You should consider upgrading via the 'c:\\users\\sandeep bhatula\\appdata\\local\\programs\\python\\python38\\python.exe -m pip install --upgrade pip' command.\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "!pip install tweepy\n",
    "!pip install sklearn\n",
    "\n",
    "import math\n",
    "import gensim.models as gs\n",
    "from gensim.models import KeyedVectors\n",
    "import pickle as pk\n",
    "import sklearn.metrics as met\n",
    "import scipy.stats as stats\n",
    "import numpy as np\n",
    "import re\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from gensim.models.fasttext import FastText\n",
    "import twitter_sentiment_dataset as tsd\n",
    "import phrase2vec as p2v\n",
    "from twitter_sentiment_dataset import TweetTrainingExample\n",
    "import urllib.request\n",
    "import io\n",
    "from gensim.test.utils import datapath\n",
    "from gensim.test.utils import common_texts\n",
    "from gensim.models import Word2Vec\n",
    "from gensim.models import KeyedVectors\n",
    "#from model import ModelParams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#w2v_path = KeyedVectors.load_word2vec_format('./pre-trained/GoogleNews-vectors-negative300.bin', binary=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setup\n",
    "Load the three vector representations from files. In general, any variable with the word 'none' in it refers to Google News word2vec w/o any emoji vectors, 'ours' to Google News word2vec w/ vectors we trained, and 'theirs' to Google News word2vec with the vectors trained by Barbieri et. al."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "w2v_path='./pre-trained/GoogleNews-vectors-negative300.bin'\n",
    "\n",
    "#print(\"Check\")\n",
    "#print(word2vec)\n",
    "in_dim = 300   # Length of word2vec vectors\n",
    "out_dim = 300  # Desired dimension of output vectors\n",
    "pos_ex = 4\n",
    "neg_ratio = 1\n",
    "max_epochs = 40\n",
    "dropout = 0.0\n",
    "\n",
    "\n",
    "e2v_ours_path = './pre-trained/emoji2vec_bow_desc.bin'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "w2v = KeyedVectors.load_word2vec_format(w2v_path, binary=True)\n",
    "e2v_ours = KeyedVectors.load_word2vec_format(e2v_ours_path, binary=True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: gensim==3.8.1 in c:\\users\\sandeep bhatula\\appdata\\local\\programs\\python\\python38\\lib\\site-packages (3.8.1)\n",
      "Requirement already satisfied: numpy>=1.11.3 in c:\\users\\sandeep bhatula\\appdata\\local\\programs\\python\\python38\\lib\\site-packages (from gensim==3.8.1) (1.20.2)\n",
      "Requirement already satisfied: six>=1.5.0 in c:\\users\\sandeep bhatula\\appdata\\roaming\\python\\python38\\site-packages (from gensim==3.8.1) (1.15.0)\n",
      "Requirement already satisfied: smart-open>=1.8.1 in c:\\users\\sandeep bhatula\\appdata\\local\\programs\\python\\python38\\lib\\site-packages (from gensim==3.8.1) (5.2.1)\n",
      "Requirement already satisfied: scipy>=0.18.1 in c:\\users\\sandeep bhatula\\appdata\\local\\programs\\python\\python38\\lib\\site-packages (from gensim==3.8.1) (1.6.3)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Value for scheme.headers does not match. Please report this to <https://github.com/pypa/pip/issues/9617>\n",
      "distutils: c:\\users\\sandeep bhatula\\appdata\\local\\programs\\python\\python38\\Include\\UNKNOWN\n",
      "sysconfig: c:\\users\\sandeep bhatula\\appdata\\local\\programs\\python\\python38\\Include\n",
      "WARNING: Additional context:\n",
      "user = False\n",
      "home = None\n",
      "root = None\n",
      "prefix = None\n",
      "WARNING: Value for scheme.headers does not match. Please report this to <https://github.com/pypa/pip/issues/9617>\n",
      "distutils: c:\\users\\sandeep bhatula\\appdata\\local\\programs\\python\\python38\\Include\\UNKNOWN\n",
      "sysconfig: c:\\users\\sandeep bhatula\\appdata\\local\\programs\\python\\python38\\Include\n",
      "WARNING: Additional context:\n",
      "user = False\n",
      "home = None\n",
      "root = None\n",
      "prefix = None\n",
      "WARNING: You are using pip version 21.1; however, version 21.3.1 is available.\n",
      "You should consider upgrading via the 'c:\\users\\sandeep bhatula\\appdata\\local\\programs\\python\\python38\\python.exe -m pip install --upgrade pip' command.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['🌈', '🌉', '🍺', '🌅', '🌆', '🍻', '🌀', '🌁', '🌂', '🌃', '🇫🇷', '💘', '💙', '💔', '💕', '💖', '💗', '💐', '💑', '💒', '💓', '⏪', '👞', '🎓', '🌄', '🎑', '🎐', '👟', '☝', '⏫', '🍼', '🌜', '🌇', '💝', '💞', '💟', '💚', '💛', '💜', '🍹', '🍸', '🍱', '🌎', '🍳', '🍲', '🍵', '🌊', '🍷', '🌌', '🚿', '🚾', '😠', '👝', '🎶', '🎷', '🎴', '🎵', '㊗', '🎳', '🎰', '🎱', '📏', '㊙', '📎', '📋', '📌', '🎸', '🎹', '💧', '🇧', '👇', '👆', '💨', '👅', '🎿', '🎽', '🎾', '🎻', '🎼', '📈', '🎺', '📆', '📇', '📄', '📅', '📂', '📃', '📀', '📁', '🌾', '🌽', '😮', '🌿', '🌺', '😫', '😪', '🌻', '👐', '👑', '👒', '👓', '👔', '👕', '👖', '👗', '🔑', '🔐', '🈺', '☁', '☀', '🎒', '🎧', '🔔', '🐶', '👌', '🎥', '🇰🇷', '🐷', '🎤', '🐴', '🆎', '🈸', '🈹', '🗿', '🈴', '🈵', '🈶', '🇬🇧', '🈲', '🐳', '😧', '😦', '🌷', '😤', '🌱', '🌰', '😡', '🌲', '👚', '👛', '👜', '🌹', '🌸', '😩', '😨', '🔠', '📟', '📞', '✴', '✳', '☎', '🔤', '⚾', '⚽', '📛', '🚂', '🔦', '❌', '⬇', '⬆', '⬅', '📝', '📜', '🔥', '📚', '🔧', '🔨', '🔩', '🌚', '🅱', '🅰', '⛄', '🔬', '⛺', '⛅', '🙌', '🏯', '🏮', '🏭', '🏬', '🏫', '🏪', '📗', '🔪', '📕', '📔', '🔭', '🔮', '📑', '📐', '▶', '🇾', '📙', '📘', '🇪🇸', '🏩', '🏨', '🏧', '🏦', '🏥', '🏤', '🏣', '🏢', '🏡', '🏠', '🇷🇺', '👄', '🐔', '🐕', '👬', '🐗', '🐐', '👭', '🚤', '👯', '🚨', '🚩', '🐘', '🐙', '⏩', '🌍', '♉', '🍰', '🌏', '⛵', '⚪', '⚫', '🎊', '⛳', '⛲', '🍴', '🌋', '🍶', '⤵', '♏', '♎', '♍', '♌', '♋', '♊', '👡', '👠', '🐟', '👢', '⏰', '🐚', '👧', '⏳', '👩', '👨', '😒', '😓', '😐', '⚠', '⚡', '🎆', '🎇', '🎀', '🎁', '🎂', '🎃', '🍄', '🎈', '🎉', '🍅', '😔', '📖', '🎬', '🎫', '🕦', '🕧', '🔞', '🕡', '🎮', '🕣', '🍈', '🍨', '🍩', '👃', '🐰', '🈂', '🍢', '🍣', '🍠', '🍡', '🍦', '🍧', '🍤', '🍥', '🎯', '😙', '🔯', '🇩🇪', '🙅', '🚊', '❇', '❄', '🚋', '🚎', '🚍', '🚏', '🙆', '🔜', '🍫', '🍬', '🍪', '🍯', '🍭', '🍮', '🔙', '🔘', '🎩', '🎨', '🎣', '🎢', '🎡', '🔒', '🔕', '🎦', '🔗', '🔖', '🚉', '🚈', '🚁', '🚀', '🚃', '❎', '🚅', '🚄', '🚇', '🚆', '😕', '➡', '😊', '😌', '™', '😋', '🐮', '🐭', '🐯', '🐪', '🐽', '🐬', '😍', '🐾', '🔫', '😏', '💆', '☺', '🐺', '⛪', '📉', '🐩', '🐨', '🐥', '🐤', '🐧', '🐦', '🐡', '🐠', '🐣', '🐢', '🔟', '◻', '◼', '◽', '◾', '🇸', '🕝', '🕟', '🕚', '👴', '🕜', '🕛', '😰', '😱', '🇲', '🇳', '😴', '🇵', '😶', '😷', '➿', '😚', '🌠', '😟', '✈', '✉', '😝', '✂', '✅', '🇯🇵', '🇴', '➰', '🎠', '🇺', '🕕', '🕔', '🕗', '🕖', '🕑', '🕐', '🕓', '🕒', '🎲', '🇻', '🇼', '🕙', '🕘', '😿', '📬', '🔢', '⏬', '🆒', '✋', '✌', '✊', '✏', '📍', '♻', '◀', '♿', '🚣', '🐖', '🅿', '🅾', '🚽', '🚼', '🚻', '🚺', '🚦', '🍌', '📊', '🚧', '🔴', '🇮🇹', '🐒', '🔓', '📪', '📫', '🐓', '📭', '📮', '📯', '🔲', '💍', '🚷', '🚶', '🚵', '🚴', '🚳', '🚲', '🚱', '🚰', '🚹', '🚸', '🇺🇸', '🙍', '🌳', '📨', '📩', '📠', '📡', '📢', '📣', '📤', '📥', '📦', '📧', '🇪', '🇬', '🇫', '😎', '🇭', '🇯', '🍟', '🍞', '🍝', '🍜', '🍛', '🍚', '®', '👲', '👳', '👰', '👱', '👶', '👷', '🚢', '👵', '👸', '👹', '😹', '👾', '🐑', '👻', '🎍', '👺', '👿', '👽', '🎎', '🇨🇳', '🎏', '😁', '😀', '😃', '😂', '😅', '😄', '🍙', '😆', '🇩', '🍖', '🍕', '🍔', '🍓', '🍒', '🍑', '🍐', '🎋', '🎌', '🚡', '❓', '©', '❕', '❔', '❗', '⚓', '😸', '🐝', '😇', '🇹', '🇦', '📓', '‼', '📺', '😉', '🔈', '📻', '🙋', '🇨', '🔂', '🔃', '🔀', '🔁', '🔆', '🔇', '🔄', '🔅', '💭', '💮', '💯', '🚙', '💪', '💫', '💬', '🚒', '🚓', '🚐', '🚑', '🚖', '🚗', '🚔', '🚕', '🇰', '🇱', '💇', '😲', '📱', '📰', '📳', '😳', '📵', '📴', '📷', '📶', '📹', '🔌', '🔊', '🔏', '🔍', '😵', '👼', '🇶', '🇷', 'ℹ', '〽', '🚌', '🚥', '💤', '💥', '💦', '⌛', '💠', '💡', '💢', '💣', '🚛', '🚜', '🚚', '🚟', '💩', '🚝', '🚞', '🀄', '🐸', '🐹', '💌', '💋', '💊', '🐵', '🐲', '💏', '💎', '🐱', '👪', '🙎', '🌛', '😈', '🙏', '🙊', '🌟', '🌞', '🌝', '⛔', '⤴', '↪', '🌙', '🌘', '🙉', '🙈', '🌓', '🌒', '🙇', '🌐', '🌗', '🙀', '🌕', '🌔', '💉', '💈', '💃', '💂', '💁', '💀', '🐻', '🐼', '💅', '💄', '🚫', '↩', '🐞', '✔', '✖', '👙', '👣', '✒', '👫', '🚪', '♒', '👥', '👤', '👘', '🐛', '😯', '♈', '👦', '👮', '😭', '🚬', '😬', '🌑', '🌼', '😽', '🏊', '❤', '🐜', '📒', '⭐', '⭕', '🏄', '🏇', '🏆', '🏁', '🏀', '🏃', '🏂', '🉐', '🏉', '📼', '🚯', '🔉', '🍃', '🌖', '👏', '👎', '👍', '🃏', '👋', '👊', '🐂', '🏈', '♥', '♦', '♠', '♣', '♨', '🎄', '😺', '⛎', '🐅', '🎅', '🚭', '💹', '💸', '🗾', '💵', '💴', '💷', '💶', '💱', '💰', '💳', '💲', '🚮', '🚘', '🆕', '🆔', '🆗', '🆖', '🆑', '👂', '🆓', '👀', '🆙', '🆘', '👉', '👈', '☑', '☔', '☕', '💾', '💽', '💿', '💺', '💼', '💻', '↗', '↖', '↕', '↔', '🛀', '🛁', '🛂', '🛃', '🛄', '🛅', '↙', '↘', '⬛', '⬜', '🔽', '🔼', '🔻', '🔺', '🍀', '🍁', '🍂', '😑', '😖', '😗', '🍆', '🍇', '🍘', '🆚', '😘', '🕤', '😻', '🇮', '😼', '🇽', '🍉', '😾', '🔚', '🇿', '🕥', '⁉', '✨', '🎪', '😛', '🍊', '🍋', '🔛', '🍍', '🍎', '🍏', '😞', '🕠', '〰', '🔝', '🕢', '🔹', '🔸', '🔷', '🔶', '🔵', '🎭', '🔳', '🈯', '🔱', '🔰', '🈚', '🐫', '🐌', '🐋', '🐊', '📲', '🐏', '🐎', '🐍', '🏰', '🍗', '🔋', '🐃', '⛽', '🐁', '🐀', '🐇', '🐆', 'Ⓜ', '🐄', '🗽', '⌚', '🐉', '🐈', '🗻', '🗼', '➕', '➗', '➖', '🚠', '▫', '▪', '🈳', '♐', '♑', '😜', '♓', '🌵', '🕞', '🌴', '😥', '🔎', '😣', '😢']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\sandeep bhatula\\appdata\\local\\programs\\python\\python38\\lib\\site-packages\\gensim\\models\\base_any2vec.py:742: UserWarning: C extension not loaded, training will be slow. Install a C compiler and reinstall gensim for fast training.\n",
      "  warnings.warn(\n",
      "C:\\Users\\SANDEE~1\\AppData\\Local\\Temp/ipykernel_15796/1121666916.py:17: DeprecationWarning: Call to deprecated `__getitem__` (Method will be removed in 4.0.0, use self.wv.__getitem__() instead).\n",
      "  values.append(np.array(wv[i]))\n"
     ]
    }
   ],
   "source": [
    "!pip install gensim==3.8.1\n",
    "wv=Word2Vec.load(str('./pre-trained/model_swm-300-3-3'))\n",
    "e2v_theirs = KeyedVectors(300)\n",
    "keys=[]\n",
    "#print(e2v_theirs.wv.vocab.keys())\n",
    "for i in wv.wv.vocab.keys():\n",
    "    if len(i)>9:\n",
    "        temp=chr(int(i[4:9],16))+chr(int(i[9:14],16))\n",
    "        keys.append(temp)\n",
    "    else:\n",
    "        temp=chr(int(i[4:9],16))\n",
    "        keys.append(temp)\n",
    "print(keys)\n",
    "#print(wv.wv.vocab.values())\n",
    "values=[]\n",
    "for i in wv.wv.vocab.keys():\n",
    "    values.append(np.array(wv[i]))\n",
    "e2v_theirs.add(entities=keys, weights=np.array(values))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#print(e2v_theirs['eoji1f308'])\n",
    "p2v_no_emoji = p2v.Phrase2Vec(out_dim, w2v, e2v=None)\n",
    "p2v_our_emoji = p2v.Phrase2Vec(out_dim, w2v, e2v=e2v_ours)\n",
    "p2v_their_emoji = p2v.Phrase2Vec(out_dim, w2v, e2v=e2v_theirs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using stats scraped from emojitracker.com at a certain point in time, we generate two sets of emoji: the top 173 most frequently used emoji, whose usage constitutes 90% of emoji usage on Twitter, and the bottom 612 least frequently used emoji, whose usage constitutes 10% of emoji usage on Twitter.\n",
    "\n",
    "Subsequently, 'common' will refer to the former group, while 'rare' will refer to the latter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "p = open('./data/tweets/frequencies_w_emoji.txt', 'r',encoding='UTF-8')\n",
    "ems = p.readlines()\n",
    "ems = [l.split('\\t')[0] for l in ems]\n",
    "p.close()\n",
    "top90 = set(ems[:173])\n",
    "bottom10 = set(ems[173:])\n",
    "p.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def emoji_dataset_stats(tweets):\n",
    "    total_tweets = len(tweets)\n",
    "    total_emoji = tsd.num_tweets_with_emoji(tweets, e2v_ours, e2v_theirs, ems)\n",
    "    top_90_total = tsd.num_tweets_with_emoji(tweets, set(), set(), top90)\n",
    "    bottom_10_total = tsd.num_tweets_with_emoji(tweets, set(), set(), bottom10)\n",
    "    return total_tweets, total_emoji, top_90_total, bottom_10_total"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Statistics for the entire Twitter corpus. Counts refer to # of tweets containing emoji of a type."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All Tweets in corpus: 64599, total emoji: 11701, common emoji: 11137, rare emoji: 1576\n",
      "Training set: total tweets: 51679, total emoji: 9405, common emoji: 8950, rare emoji: 1268\n",
      "Test set: total tweets: 12920, total emoji: 2296, common emoji: 2187, rare emoji: 308\n"
     ]
    }
   ],
   "source": [
    "train_tweets, test_tweets = tsd.load_training_test_sets()\n",
    "print('All Tweets in corpus: %s, total emoji: %s, common emoji: %s, rare emoji: %s' % emoji_dataset_stats(tsd.get_all_examples()))\n",
    "print('Training set: total tweets: %s, total emoji: %s, common emoji: %s, rare emoji: %s' % emoji_dataset_stats(train_tweets))\n",
    "print('Test set: total tweets: %s, total emoji: %s, common emoji: %s, rare emoji: %s' % emoji_dataset_stats(test_tweets))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "def emoji_dataset_label_stats(tweets):\n",
    "    res = dict()\n",
    "    res['Positive'] = 0\n",
    "    res['Negative'] = 0\n",
    "    res['Neutral'] = 0\n",
    "    for tweet in tweets:\n",
    "        res[tweet.label] += 1/len(tweets)\n",
    "    print(res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Positive': 0.2882215213142463, 'Negative': 0.25207531105472025, 'Neutral': 0.45970316763111974}\n",
      "{'Positive': 0.2876160990712081, 'Negative': 0.25069659442723424, 'Neutral': 0.4616873065016006}\n"
     ]
    }
   ],
   "source": [
    "emoji_dataset_label_stats(train_tweets)\n",
    "emoji_dataset_label_stats(test_tweets)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare Training and Testing Vectors\n",
    "Given the raw training and test tweets, calculate the vector representations for each tweet for each model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "train_none, train_y = tsd.prepare_tweet_vector_averages(train_tweets, p2v_no_emoji)\n",
    "train_ours, _ = tsd.prepare_tweet_vector_averages(train_tweets, p2v_our_emoji)\n",
    "train_theirs, _ = tsd.prepare_tweet_vector_averages(train_tweets, p2v_their_emoji)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "test_none, test_y = tsd.prepare_tweet_vector_averages(test_tweets, p2v_no_emoji)\n",
    "test_ours, _ = tsd.prepare_tweet_vector_averages(test_tweets, p2v_our_emoji)\n",
    "test_theirs, _ = tsd.prepare_tweet_vector_averages(test_tweets, p2v_their_emoji)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "classifiers = {\n",
    "    'SGD(n_iter=50) ' : SGDClassifier(max_iter=50,random_state=1234),\n",
    "    'Random Forest (n_estimators=60)' : RandomForestClassifier(n_estimators=60,random_state=1234)\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def train_all_with_cross_validation(train_none, train_ours, train_theirs, train_y, clf, clf_name, cv=5):\n",
    "    scores_none = cross_val_score(clf, train_none, train_y, cv=cv)\n",
    "    print(\"None: %s Train Accuracy: %0.2f (+/- %0.3f)\" % (clf_name, scores_none.mean(), scores_none.std() * 2))\n",
    "    \n",
    "    scores_ours = cross_val_score(clf, train_ours, train_y, cv=cv)\n",
    "    print(\"Ours: %s Train Accuracy: %0.2f (+/- %0.3f)\" % (clf_name, scores_ours.mean(), scores_ours.std() * 2))\n",
    "    \n",
    "    scores_theirs = cross_val_score(clf, train_theirs, train_y, cv=cv)\n",
    "    print(\"Theirs: %s Train Accuracy: %0.2f (+/- %0.3f)\" % (clf_name, scores_theirs.mean(), scores_theirs.std() * 2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def train_and_predict(train_data, train_y, test_data, test_y, clf):\n",
    "    clf.fit(train_data, train_y)\n",
    "    predictions = clf.predict(test_data)\n",
    "    score = met.accuracy_score(test_y, predictions)\n",
    "    f1 = met.f1_score(test_y, predictions, average='weighted')\n",
    "    return predictions, score, f1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def train_and_predict_all(train_none, test_none, train_ours, test_ours, train_theirs, test_theirs, test_y, clf, clf_name):\n",
    "    none_pred, none_acc, none_f1 = train_and_predict(train_none, train_y, test_none, test_y, clf)\n",
    "    print('None: %s Test Accuracy: %0.5f, f1=%0.5f' % (clf_name, none_acc, none_f1))\n",
    "    \n",
    "    ours_pred, ours_acc, ours_f1 = train_and_predict(train_ours, train_y, test_ours, test_y, clf)\n",
    "    ours_p = tsd.calculate_mcnemars(none_pred, ours_pred, test_y)\n",
    "    print('Ours: %s Test Accuracy: %0.5f, p=%0.5f, f1=%0.5f' % (clf_name, ours_acc, ours_p, ours_f1))\n",
    "    \n",
    "    theirs_pred, theirs_acc, theirs_f1 = train_and_predict(train_theirs, train_y, test_theirs, test_y, clf)\n",
    "    theirs_p = tsd.calculate_mcnemars(none_pred, theirs_pred, test_y)\n",
    "    print('Theirs: %s Test Accuracy: %0.5f, p=%0.5f, f1=%0.5f' % (clf_name, theirs_acc, theirs_p, theirs_f1))\n",
    "    \n",
    "    ours_theirs_p = tsd.calculate_mcnemars(ours_pred, theirs_pred, test_y)\n",
    "    print('Significance between ours and theirs: p=%0.5f' % ours_theirs_p)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Performance on Training Set and Complete Test Set\n",
    "For each classifier, we calculate the average performance of the classifier on the training set when cross validation is applied, as well as the accuracy on the complete test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SGD(n_iter=50) \n",
      "\n",
      "Cross Validation Accuracy on Training Set\n",
      "\n",
      "None: SGD(n_iter=50)  Train Accuracy: 0.61 (+/- 0.018)\n",
      "Ours: SGD(n_iter=50)  Train Accuracy: 0.61 (+/- 0.018)\n",
      "Theirs: SGD(n_iter=50)  Train Accuracy: 0.62 (+/- 0.018)\n",
      "\n",
      "Accuracy on Test Set\n",
      "\n",
      "None: SGD(n_iter=50)  Test Accuracy: 0.61525, f1=0.60497\n",
      "Ours: SGD(n_iter=50)  Test Accuracy: 0.61780, p=0.43450, f1=0.60793\n",
      "Theirs: SGD(n_iter=50)  Test Accuracy: 0.61594, p=0.80958, f1=0.60147\n",
      "Significance between ours and theirs: p=0.50564\n",
      "\n",
      "Random Forest (n_estimators=60)\n",
      "\n",
      "Cross Validation Accuracy on Training Set\n",
      "\n",
      "None: Random Forest (n_estimators=60) Train Accuracy: 0.58 (+/- 0.010)\n",
      "Ours: Random Forest (n_estimators=60) Train Accuracy: 0.59 (+/- 0.006)\n",
      "Theirs: Random Forest (n_estimators=60) Train Accuracy: 0.58 (+/- 0.008)\n",
      "\n",
      "Accuracy on Test Set\n",
      "\n",
      "None: Random Forest (n_estimators=60) Test Accuracy: 0.58235, f1=0.56633\n",
      "Ours: Random Forest (n_estimators=60) Test Accuracy: 0.59420, p=0.00449, f1=0.58178\n",
      "Theirs: Random Forest (n_estimators=60) Test Accuracy: 0.59156, p=0.01801, f1=0.57614\n",
      "Significance between ours and theirs: p=0.50621\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for clf_name, clf in classifiers.items():\n",
    "    print(clf_name)\n",
    "    \n",
    "    print()\n",
    "    \n",
    "    print('Cross Validation Accuracy on Training Set\\n')\n",
    "    train_all_with_cross_validation(train_none, train_ours, train_theirs, train_y, clf, clf_name, cv=5)\n",
    "    \n",
    "    print()\n",
    "    \n",
    "    print('Accuracy on Test Set\\n')\n",
    "    train_and_predict_all(train_none, test_none, train_ours, test_ours, train_theirs, test_theirs, test_y, clf, clf_name)\n",
    "    \n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def train_and_predict_all_on_test_subset(test_tweets, clf, clf_name):\n",
    "    test_none, test_y = tsd.prepare_tweet_vector_averages(test_tweets, p2v_no_emoji)\n",
    "    test_ours, _ = tsd.prepare_tweet_vector_averages(test_tweets, p2v_our_emoji)\n",
    "    test_theirs, _ = tsd.prepare_tweet_vector_averages(test_tweets, p2v_their_emoji)\n",
    "\n",
    "    train_and_predict_all(train_none, test_none, train_ours, test_ours, train_theirs, test_theirs, test_y, clf, clf_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "emoji_test_tweets = tsd.get_tweets_with_emoji(test_tweets, e2v_ours, e2v_theirs, ems)\n",
    "emoji_test_tweets_top90 = tsd.get_tweets_with_emoji(test_tweets, set(), set(), top90)\n",
    "emoji_test_tweets_bottom10 = tsd.get_tweets_with_emoji(test_tweets, set(), set(), bottom10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test Subset - All Tweets with Emoji\n",
    "For each classifier, we calculate the accuracy on the subset of test examples that contain emoji."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SGD(n_iter=50) \n",
      "None: SGD(n_iter=50)  Test Accuracy: 0.51045, f1=0.52333\n",
      "Ours: SGD(n_iter=50)  Test Accuracy: 0.61237, p=0.00000, f1=0.60779\n",
      "Theirs: SGD(n_iter=50)  Test Accuracy: 0.58057, p=0.00000, f1=0.59213\n",
      "Significance between ours and theirs: p=0.00180\n",
      "\n",
      "Random Forest (n_estimators=60)\n",
      "None: Random Forest (n_estimators=60) Test Accuracy: 0.47997, f1=0.49073\n",
      "Ours: Random Forest (n_estimators=60) Test Accuracy: 0.58145, p=0.00000, f1=0.57551\n",
      "Theirs: Random Forest (n_estimators=60) Test Accuracy: 0.52831, p=0.00001, f1=0.54418\n",
      "Significance between ours and theirs: p=0.00003\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for clf_name, clf in classifiers.items():\n",
    "    print(clf_name)\n",
    "    train_and_predict_all_on_test_subset(emoji_test_tweets, clf, clf_name)\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test Subset - All Tweets with Common Emoji\n",
    "For each classifier, we calculate the accuracy on the subset of test examples that contain common (Top 90%) emoji."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SGD(n_iter=50) \n",
      "None: SGD(n_iter=50)  Test Accuracy: 0.51212, f1=0.52588\n",
      "Ours: SGD(n_iter=50)  Test Accuracy: 0.61774, p=0.00000, f1=0.61200\n",
      "Theirs: SGD(n_iter=50)  Test Accuracy: 0.58390, p=0.00000, f1=0.59546\n",
      "Significance between ours and theirs: p=0.00110\n",
      "\n",
      "Random Forest (n_estimators=60)\n",
      "None: Random Forest (n_estimators=60) Test Accuracy: 0.48194, f1=0.49364\n",
      "Ours: Random Forest (n_estimators=60) Test Accuracy: 0.59214, p=0.00000, f1=0.58457\n",
      "Theirs: Random Forest (n_estimators=60) Test Accuracy: 0.53132, p=0.00001, f1=0.54766\n",
      "Significance between ours and theirs: p=0.00000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for clf_name, clf in classifiers.items():\n",
    "    print(clf_name)\n",
    "    train_and_predict_all_on_test_subset(emoji_test_tweets_top90, clf, clf_name)\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test Subset - All Tweets with Rare Emoji\n",
    "For each classifier, we calculate the accuracy on the subset of test examples that contain rare (Bottom 10%) emoji."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SGD(n_iter=50) \n",
      "None: SGD(n_iter=50)  Test Accuracy: 0.45779, f1=0.45660\n",
      "Ours: SGD(n_iter=50)  Test Accuracy: 0.56494, p=0.00341, f1=0.56746\n",
      "Theirs: SGD(n_iter=50)  Test Accuracy: 0.54545, p=0.00238, f1=0.55093\n",
      "Significance between ours and theirs: p=0.52243\n",
      "\n",
      "Random Forest (n_estimators=60)\n"
     ]
    }
   ],
   "source": [
    "for clf_name, clf in classifiers.items():\n",
    "    print(clf_name)\n",
    "    train_and_predict_all_on_test_subset(emoji_test_tweets_bottom10, clf, clf_name)\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
